{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOCIAL MEDIA ANALYSER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SanketHegde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SanketHegde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SanketHegde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\SanketHegde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Facebook Graph API setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pip install facebook-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'created_time': '2023-12-20T16:43:14+0000', 'from': {'name': 'Test page', 'id': '212279481958718'}, 'message': 'amazing scenery', 'id': '122107628540149586_890844895781676'}, {'created_time': '2023-12-20T16:42:59+0000', 'from': {'name': 'Test page', 'id': '212279481958718'}, 'message': 'looks beautiful', 'id': '122107628540149586_880098136994384'}, {'created_time': '2023-12-20T16:43:18+0000', 'from': {'name': 'Test page', 'id': '212279481958718'}, 'message': 'the best', 'id': '122107628540149586_685719837032141'}, {'created_time': '2023-12-20T16:43:01+0000', 'from': {'name': 'Test page', 'id': '212279481958718'}, 'message': 'wow', 'id': '122107628540149586_740407807518240'}, {'created_time': '2023-12-20T16:43:25+0000', 'from': {'name': 'Test page', 'id': '212279481958718'}, 'message': 'i hate it', 'id': '122107628540149586_766511461976242'}, {'created_time': '2023-12-20T16:43:22+0000', 'from': {'name': 'Test page', 'id': '212279481958718'}, 'message': 'its trash', 'id': '122107628540149586_171244586079999'}], 'paging': {'cursors': {'before': 'NgZDZD', 'after': 'MQZDZD'}}}\n",
      "<class 'str'>\n",
      "Comments written to sample.txt\n"
     ]
    }
   ],
   "source": [
    "page_id = \"61554487605477\"              # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<  Enter the page id here\n",
    "post_id = \"122107628540149586\"          # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<  Enter the post id here\n",
    "\n",
    "access_token = 'EAAPaIduWx3MBO63ic5liYYxmXCa8b4jDZBRcPJdivCItf2iRHvUdAOZA2XzXrtbq8mcAZCvz8P70D2wYNTzuhZBn5xkgKFZBgrAwueOUstsvwd52sRrGuFqGyjsdUL8ypMuTfY3Y5gZAif7Jk48hysA9pkhapIb7El6Io0QXXwt7f48JRfvO80ONQsPveAumPIUB0yM2pmNsjyvNo3f5zqtsdEy4h9JvUZD' \n",
    "# your access token, from https://developers.facebook.com/tools/explorer/\n",
    "\n",
    "url = f'https://graph.facebook.com/v16.0/{page_id}_{post_id}/comments?access_token={access_token}'\n",
    "\n",
    "response = requests.request(\"GET\", url)\n",
    "\n",
    "# save name, time, message in excel file\n",
    "data = json.loads(response.text)\n",
    "\n",
    "# create object with only name, time, message\n",
    "print(data)\n",
    "def get_comment_text(comment):\n",
    "    return f'{comment[\"message\"]} '\n",
    "\n",
    "# Create a string containing all comments\n",
    "text_data = '\\n'.join(map(get_comment_text, data.get('data', [])))\n",
    "\n",
    "# Write the comments to a text file\n",
    "with open(r'C:\\Users\\SanketHegde\\Desktop\\Final Year Project\\sample.txt', 'w', encoding='utf-8') as text_file:\n",
    "    text_file.write(text_data)\n",
    "\n",
    "# Display success message\n",
    "print(type(text_data))\n",
    "\n",
    "print('Comments written to sample.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Youtube-API setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import googleapiclient.discovery\n",
    "# import googleapiclient.errors\n",
    "\n",
    "# api_service_name = \"youtube\"\n",
    "# api_version = \"v3\"\n",
    "# DEVELOPER_KEY = \"AIzaSyBWZoWKKtgc5s-oHTE5P_l0FwafRENBkow\"\n",
    "\n",
    "# VIDEO_ID = \"-1gF6PIB1YQ\"      #    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<  Enter the video id here            \n",
    "# youtube = googleapiclient.discovery.build(\n",
    "#     api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "# request = youtube.commentThreads().list(\n",
    "#     part=\"snippet\",\n",
    "#     videoId=VIDEO_ID,\n",
    "#     maxResults=100\n",
    "# )\n",
    "# response = request.execute()\n",
    "\n",
    "# text = ''\n",
    "\n",
    "# for item in response['items']:\n",
    "#     # print(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "#     text += item['snippet']['topLevelComment']['snippet']['textDisplay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** feed Data from Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample.txt', encoding ='ISO-8859-2') as f:\n",
    "#     text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tokenize - split words from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = PunktSentenceTokenizer(text_data)\n",
    "sents = sent_tokenizer.tokenize(text_data)\n",
    "\n",
    "# print(word_tokenize(text))\n",
    "# print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For stemize we use PorterStemmer() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    " \n",
    "nltk_tokens = nltk.word_tokenize(text_data)\n",
    " \n",
    "# for w in nltk_tokens:\n",
    "#     print (\"Actual: % s, Stem: % s\" % (w, porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For lemmatize we use WordNetLemmatizer() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk_tokens = nltk.word_tokenize(text_data)\n",
    " \n",
    "# for w in nltk_tokens:\n",
    "#     print (\"Actual: % s, Lemma: % s\" % (w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** POS( part of speech) tagging of the tokens and select only significant features/tokens like adjectives, adverbs, and verbs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(text_data)\n",
    "# print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** how vader sentiment analyzer works:\n",
    "\n",
    "VADER uses a combination of a sentiment lexicon which is a list of lexical features (e.g., words) which are generally labeled according to their semantic orientation as either positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazing scenery \n",
      "compound: 0.5859neg: 0.0neu: 0.208pos: 0.792\n",
      "\n",
      "\n",
      "looks beautiful \n",
      "compound: 0.5994neg: 0.0neu: 0.204pos: 0.796\n",
      "\n",
      "\n",
      "the best \n",
      "compound: 0.6369neg: 0.0neu: 0.192pos: 0.808\n",
      "\n",
      "\n",
      "wow \n",
      "compound: 0.5859neg: 0.0neu: 0.0pos: 1.0\n",
      "\n",
      "\n",
      "i hate it \n",
      "compound: -0.5719neg: 0.787neu: 0.213pos: 0.0\n",
      "\n",
      "\n",
      "its trash \n",
      "compound: 0.0neg: 0.0neu: 1.0pos: 0.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(type(text))\n",
    "# print(text)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer() \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    " \n",
    "with open(r'C:\\Users\\SanketHegde\\Desktop\\Final Year Project\\sample.txt', encoding ='ISO-8859-2') as f:\n",
    "    for comment in f.read().split('\\n'):\n",
    "        print(comment)\n",
    "        scores = sid.polarity_scores(comment)\n",
    "        for key in sorted(scores):\n",
    "            print('{0}: {1}'.format(key, scores[key]), end ='')\n",
    "        print('\\n\\n') \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Compound : 15.299999999999999 % \n",
      " Negative : 8.33 % \n",
      " Neutral  : 50.0 % \n",
      " Positive : 33.33 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer() \n",
    "\n",
    "# Initialize variables for aggregation\n",
    "total_compound = 0\n",
    "total_neg = 0\n",
    "total_neu = 0\n",
    "total_pos = 0\n",
    "total_comments = len(text)\n",
    "\n",
    "# Analyze sentiment for each string in the list\n",
    "for line in text:\n",
    "    scores = sid.polarity_scores(line)\n",
    "    total_compound += scores['compound']\n",
    "    total_neg += scores['neg']\n",
    "    total_neu += scores['neu']\n",
    "    total_pos += scores['pos']\n",
    "\n",
    "    # Print the individual sentiment scores for each comment\n",
    "    # print(f\"{line}\\nCompound: {scores['compound']}, Negative: {scores['neg']}, Neutral: {scores['neu']}, Positive: {scores['pos']}\\n\")\n",
    "\n",
    "# Calculate averages\n",
    "average_compound = round(total_compound / total_comments, 4) * 100\n",
    "average_neg = round(total_neg / total_comments, 4) * 100\n",
    "average_neu = round(total_neu / total_comments, 4) * 100\n",
    "average_pos = round(total_pos / total_comments, 4) * 100\n",
    "\n",
    "# Print the aggregated sentiment scores\n",
    "print(f\" Compound : {average_compound} % \\n Negative : {average_neg} % \\n Neutral  : {average_neu} % \\n Positive : {average_pos} %\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
